# Abstract
This paper explores BERT in the context of learning to rank. Two new models, monoBERT and duoBERT are proposed to extract documents relevant to a query.

# Multi-Stage Ranking Architecture
![image](https://github.com/dvksn/papers-summary/assets/18422658/531b57de-c3cf-497f-83c9-427245cc130b)

# Let's understand each component
## BM25
The query is treated as bag-of-words and inverted-index based on BM25 score is used to get relevant documents. BM25 looks for exact word match and gives lower score to synonyms. That's why recall is more important in this stage as later stages will increase the precision.
## monoBERT
This can be considered as point wise prediction task from learning-to-rank framework. A pair of query and document is sent to BERT model. And the model predicts a relevancy score for this document. Since, BERT is trrained to handle 512 tokens only, the authors truncates query to have 64 tokens at max. Documents are also truncated to fit (query, document) pair in 512 tokens. 
[CLS] token value is passed through a single layer network to get probability si of document di being relevant to query q.
A new list of candidates is generated by selecting top-k0 candidates based on this probability.
### Loss function
This model is trained on cross entropy loss.
![image](https://github.com/dvksn/papers-summary/assets/18422658/263fa809-f961-49b4-8737-9eef33f6d5ad)
## duoBERT
It is a pairwise ranker model. Input is the candidate list from monoBERT model. For each query, a triplet {query, doc1, doc2} is created and sent to BERT model. query, doc1 and doc2 are separated by [SEP] token and different segement embedding is added to position embedding and token embedding for each token.
To fit the 512 token limit, query size is limited to 62 tokens and documents are each kept under 223 tokens.
Similar to monoBERT, [CLS] token is passed through a single layer neural network to predict p(i,j) which is the probability of document i being more relevant to query than document j.
Following loss function is used:
![image](https://github.com/dvksn/papers-summary/assets/18422658/c8f8ba2b-6c7e-40b4-aeee-3a177853cffa)
At the inference time, p(i,j) is aggregated over all j to get a single score si for ith document. The ranking is done by sorting documents using score s.

# Training data
Authors used following datasets
1.  MS MARCO: 500k pairs of query, relevent docs and 400M pairs of query, non-relevant docs are used at training.
2.  TREC CAR: This data is created using hierarchical structure of Wikipedia where query is constructed by concatenating a Wikipedia article title with the title of one of its sections. The relevant documents are paragraph within that section.

